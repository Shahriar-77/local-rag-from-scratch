{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "743e0b4a",
   "metadata": {},
   "source": [
    "# DocInsight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799c2885",
   "metadata": {},
   "source": [
    "- The first goal of this project is to build a working IR (Information Retrieval) system that can work with some of the most common file formats (csv, txt, docx, pptx, xlsx, pdf).\n",
    "- The second goal will be to incorporate elements of the IR system to build a RAG (Retrieval-Augmented Generation) system with acceptable results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b166bb",
   "metadata": {},
   "source": [
    "# File Ingestion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4cc0b7",
   "metadata": {},
   "source": [
    "Below are the functions that will be used to ingest (read and store the data from the files).\n",
    "\n",
    "Each file will return the data in the form of a dict with params:\n",
    "- source_path\n",
    "- file_type\n",
    "- blocks      (structured content units)\n",
    "- text        (flattened debug view)\n",
    "- metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4bdfab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from pathlib import Path\n",
    "core_dir = Path.cwd() / 'core' \n",
    "core_dir.mkdir(exist_ok=True)\n",
    "core_dir.is_dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81db2037",
   "metadata": {},
   "source": [
    "Uncomment the line below to create the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7908aba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting core/ingestion.py\n"
     ]
    }
   ],
   "source": [
    "#%%writefile core/ingestion.py\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "from pypdf import PdfReader\n",
    "from docx import Document\n",
    "from pptx import Presentation\n",
    "from pptx.enum.shapes import PP_PLACEHOLDER\n",
    "import pandas as pd\n",
    "\n",
    "class BaseIngestor(ABC):\n",
    "    def __init__(self, file_path: Path):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    @abstractmethod\n",
    "    def extract(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Returns a dictionary with:\n",
    "        - source_path\n",
    "        - file_type\n",
    "        - blocks      (structured content units)\n",
    "        - text        (flattened debug view)\n",
    "        - metadata\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class PDFIngestor(BaseIngestor):\n",
    "    def extract(self) -> Dict:\n",
    "        reader = PdfReader(self.file_path)\n",
    "        blocks: List[Dict] = []\n",
    "\n",
    "        for page_idx, page in enumerate(reader.pages):\n",
    "            text = page.extract_text() or \"\"\n",
    "            if text.strip():\n",
    "                blocks.append({\n",
    "                    \"type\": \"page\",\n",
    "                    \"page_index\": page_idx,\n",
    "                    \"text\": text\n",
    "                })\n",
    "\n",
    "        flattened_text = \"\\n\\n\".join(b[\"text\"] for b in blocks)\n",
    "\n",
    "        return {\n",
    "            \"source_path\": str(self.file_path),\n",
    "            \"file_type\": \"pdf\",\n",
    "            \"blocks\": blocks,\n",
    "            \"text\": flattened_text,\n",
    "            \"metadata\": {\n",
    "                \"num_pages\": len(reader.pages)\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "class DOCXIngestor(BaseIngestor):\n",
    "    def extract(self) -> Dict:\n",
    "        doc = Document(self.file_path)\n",
    "        blocks: List[Dict] = []\n",
    "\n",
    "        for p in doc.paragraphs:\n",
    "            text = p.text.strip()\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            style_name = p.style.name if p.style else \"Normal\"\n",
    "\n",
    "            if style_name.startswith(\"Heading\"):\n",
    "                try:\n",
    "                    level = int(style_name.split()[-1])\n",
    "                except ValueError:\n",
    "                    level = None\n",
    "\n",
    "                blocks.append({\n",
    "                    \"type\": \"heading\",\n",
    "                    \"level\": level,\n",
    "                    \"text\": text\n",
    "                })\n",
    "            else:\n",
    "                blocks.append({\n",
    "                    \"type\": \"paragraph\",\n",
    "                    \"text\": text\n",
    "                })\n",
    "\n",
    "        flattened_text = \"\\n\".join(b[\"text\"] for b in blocks)\n",
    "\n",
    "        return {\n",
    "            \"source_path\": str(self.file_path),\n",
    "            \"file_type\": \"docx\",\n",
    "            \"blocks\": blocks,\n",
    "            \"text\": flattened_text,\n",
    "            \"metadata\": {\n",
    "                \"num_blocks\": len(blocks)\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "class PPTXIngestor(BaseIngestor):\n",
    "    def extract(self) -> Dict:\n",
    "        prs = Presentation(self.file_path)\n",
    "        blocks: List[Dict] = []\n",
    "\n",
    "        for slide_idx, slide in enumerate(prs.slides):\n",
    "            slide_elements = []\n",
    "\n",
    "            for shape in slide.shapes:\n",
    "                if not hasattr(shape, \"text\"):\n",
    "                    continue\n",
    "\n",
    "                text = shape.text.strip()\n",
    "                if not text:\n",
    "                    continue\n",
    "\n",
    "                role = \"body\"\n",
    "\n",
    "                if shape.is_placeholder:\n",
    "                    ph_type = shape.placeholder_format.type\n",
    "                    if ph_type == PP_PLACEHOLDER.TITLE:\n",
    "                        role = \"title\"\n",
    "\n",
    "                slide_elements.append({\n",
    "                    \"role\": role,\n",
    "                    \"text\": text\n",
    "                })\n",
    "\n",
    "            if slide_elements:\n",
    "                blocks.append({\n",
    "                    \"type\": \"slide\",\n",
    "                    \"slide_index\": slide_idx,\n",
    "                    \"elements\": slide_elements\n",
    "                })\n",
    "\n",
    "        flattened_text = \"\\n\\n\".join(\n",
    "            el[\"text\"]\n",
    "            for slide in blocks\n",
    "            for el in slide[\"elements\"]\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"source_path\": str(self.file_path),\n",
    "            \"file_type\": \"pptx\",\n",
    "            \"blocks\": blocks,\n",
    "            \"text\": flattened_text,\n",
    "            \"metadata\": {\n",
    "                \"num_slides\": len(prs.slides)\n",
    "            }\n",
    "        }\n",
    "\n",
    "class CSVIngestor(BaseIngestor):\n",
    "    def extract(self) -> Dict:\n",
    "        df = pd.read_csv(self.file_path)\n",
    "\n",
    "        schema = {}\n",
    "        for col in df.columns:\n",
    "            series = df[col]\n",
    "\n",
    "            inferred_type = str(series.dtype)\n",
    "\n",
    "            # crude but effective datetime detection\n",
    "            if inferred_type == \"object\":\n",
    "                sample = series.dropna().iloc[:5]\n",
    "\n",
    "                if not sample.empty:\n",
    "                    try:\n",
    "                        pd.to_datetime(sample, errors=\"raise\")\n",
    "                        inferred_type = \"datetime\"\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "\n",
    "\n",
    "            schema[col] = inferred_type\n",
    "\n",
    "        blocks = [{\n",
    "            \"type\": \"table\",\n",
    "            \"rows\": df.to_dict(orient=\"records\")\n",
    "        }]\n",
    "\n",
    "        flattened_text = df.to_csv(index=False)\n",
    "\n",
    "        return {\n",
    "            \"source_path\": str(self.file_path),\n",
    "            \"file_type\": \"csv\",\n",
    "            \"blocks\": blocks,\n",
    "            \"text\": flattened_text,\n",
    "            \"metadata\": {\n",
    "                \"num_rows\": df.shape[0],\n",
    "                \"num_columns\": df.shape[1],\n",
    "                \"schema\": schema\n",
    "            }\n",
    "        }\n",
    "    \n",
    "\n",
    "class TXTIngestor(BaseIngestor):\n",
    "    def extract(self) -> Dict:\n",
    "        content = self.file_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "        blocks = [{\n",
    "            \"type\": \"text\",\n",
    "            \"text\": content\n",
    "        }]\n",
    "\n",
    "        return {\n",
    "            \"source_path\": str(self.file_path),\n",
    "            \"file_type\": \"txt\",\n",
    "            \"blocks\": blocks,\n",
    "            \"text\": content,\n",
    "            \"metadata\": {\n",
    "                \"num_characters\": len(content)\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "class XLSXIngestor(BaseIngestor):\n",
    "    def extract(self) -> dict:\n",
    "        xls = pd.ExcelFile(self.file_path)\n",
    "        blocks = []\n",
    "\n",
    "        for sheet_name in xls.sheet_names:\n",
    "            df = xls.parse(sheet_name)\n",
    "\n",
    "            blocks.append({\n",
    "                \"type\": \"sheet\",\n",
    "                \"sheet_name\": sheet_name,\n",
    "                \"dataframe\": df\n",
    "            })\n",
    "\n",
    "        combined_text = \"\\n\\n\".join(\n",
    "            f\"Sheet: {b['sheet_name']}\\n{b['dataframe'].to_csv(index=False)}\"\n",
    "            for b in blocks\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"source_path\": str(self.file_path),\n",
    "            \"file_type\": \"xlsx\",\n",
    "            \"blocks\": blocks,\n",
    "            \"text\": combined_text,\n",
    "            \"metadata\": {\n",
    "                \"num_sheets\": len(blocks),\n",
    "                \"sheet_names\": xls.sheet_names\n",
    "            }\n",
    "        }\n",
    "    \n",
    "def get_ingestor(file_path: Path):\n",
    "    suffix = file_path.suffix.lower()\n",
    "\n",
    "    if suffix == \".pdf\":\n",
    "        return PDFIngestor(file_path)\n",
    "    elif suffix == \".docx\":\n",
    "        return DOCXIngestor(file_path)\n",
    "    elif suffix == \".pptx\":\n",
    "        return PPTXIngestor(file_path)\n",
    "    elif suffix == \".csv\":\n",
    "        return CSVIngestor(file_path)\n",
    "    elif suffix == \".txt\":\n",
    "        return TXTIngestor(file_path)\n",
    "    elif suffix =='.xlsx':\n",
    "        return XLSXIngestor(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {suffix}\")\n",
    "    \n",
    "\n",
    "def ingest_file(path_str: str):\n",
    "    path = Path(path_str)\n",
    "\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "\n",
    "    ingestor = get_ingestor(path)\n",
    "    document = ingestor.extract()\n",
    "\n",
    "    return document\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f650fe",
   "metadata": {},
   "source": [
    "Let's test the ingestion functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a80151e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Ingested: data\\raw\\sample.pdf\n",
      "Type: pdf\n",
      "Metadata: {'num_pages': 3}\n",
      "Preview:\n",
      "Annual Sales Performance Review — 2023 \n",
      "This document provides a detailed review of sales performance for the fiscal year 2023. \n",
      "The objective is to analyze revenue trends, regional performance, and category-level \n",
      "breakdowns. \n",
      "Key goals of this report include: \n",
      "• Identifying revenue growth patterns over time \n",
      "• Comparing regional performance \n",
      "• Highlighting underperforming product categories \n",
      "  \n",
      "\n",
      "Quarterly Revenue Overview \n",
      " \n",
      "Total revenue for 2023 showed a steady upward trend, with notable acc\n",
      "================================================================================\n",
      "Ingested: data\\raw\\sample.docx\n",
      "Type: docx\n",
      "Metadata: {'num_blocks': 19}\n",
      "Preview:\n",
      "Customer Churn Analysis — Internal Memo\n",
      "Background\n",
      "Customer churn has become a growing concern over the past two years.\n",
      "Understanding churn patterns is critical for improving retention strategies.\n",
      "Data Sources\n",
      "The analysis is based on the following sources:\n",
      "Monthly customer subscription records\n",
      "Support ticket logs\n",
      "Customer satisfaction survey results\n",
      "Each data source contributes a different perspective on customer behavior.\n",
      "Key Findings\n",
      "The analysis identified several notable patterns:\n",
      "Customers\n",
      "================================================================================\n",
      "Ingested: data\\raw\\sample.pptx\n",
      "Type: pptx\n",
      "Metadata: {'num_slides': 5}\n",
      "Preview:\n",
      "2023 Product Performance Summary\n",
      "\n",
      "Overview of key performance indicators\n",
      "Focus on revenue and customer adoption\n",
      "Comparison across product categories\n",
      "\n",
      "Top Performing Products\n",
      "\n",
      "Product A showed consistent growth\n",
      "Product B peaked in mid-year\n",
      "Product C underperformed expectations\n",
      "\n",
      "Revenue by Product Category\n",
      "\n",
      "Quarterly Trends\n",
      "\n",
      "Q1 and Q2 growth was moderate\n",
      "Q3 saw a sharp increase\n",
      "Q4 stabilized at a higher baseline\n",
      "\n",
      "Key Takeaways\n",
      "\n",
      "Electronics remain the primary revenue driver\n",
      "Accessories require stra\n",
      "================================================================================\n",
      "Ingested: data\\raw\\sample.csv\n",
      "Type: csv\n",
      "Metadata: {'num_rows': 9, 'num_columns': 5, 'schema': {'date': 'datetime', 'region': 'object', 'product_category': 'object', 'revenue': 'int64', 'units_sold': 'int64'}}\n",
      "Preview:\n",
      "date,region,product_category,revenue,units_sold\n",
      "2023-01-01,North America,Electronics,1200000,3400\n",
      "2023-02-01,North America,Electronics,1250000,3600\n",
      "2023-03-01,Europe,Home Appliances,980000,2100\n",
      "2023-04-01,Europe,Home Appliances,1020000,2300\n",
      "2023-05-01,Asia-Pacific,Accessories,640000,4100\n",
      "2023-06-01,Asia-Pacific,Accessories,690000,4500\n",
      "2023-07-01,North America,Electronics,1500000,4200\n",
      "2023-08-01,Europe,Electronics,1100000,3000\n",
      "2023-09-01,Asia-Pacific,Electronics,980000,2700\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\islam\\AppData\\Local\\Temp\\ipykernel_39280\\1653241032.py:164: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(sample, errors=\"raise\")\n",
      "C:\\Users\\islam\\AppData\\Local\\Temp\\ipykernel_39280\\1653241032.py:164: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(sample, errors=\"raise\")\n"
     ]
    }
   ],
   "source": [
    "sample_files = [\n",
    "        \"data/raw/sample.pdf\",\n",
    "        \"data/raw/sample.docx\",\n",
    "        \"data/raw/sample.pptx\",\n",
    "        \"data/raw/sample.csv\",\n",
    "        \n",
    "    ]\n",
    "\n",
    "for file in sample_files:\n",
    "        try:\n",
    "            doc = ingest_file(file)\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"Ingested: {doc['source_path']}\")\n",
    "            print(f\"Type: {doc['file_type']}\")\n",
    "            print(f\"Metadata: {doc['metadata']}\")\n",
    "            print(f\"Preview:\\n{doc['text'][:500]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed on {file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3597cb8",
   "metadata": {},
   "source": [
    "To see them indiviudally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0dc0a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "866da647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source_path': 'data\\\\raw\\\\sample.pdf',\n",
       " 'file_type': 'pdf',\n",
       " 'blocks': [{'type': 'page',\n",
       "   'page_index': 0,\n",
       "   'text': 'Annual Sales Performance Review — 2023 \\nThis document provides a detailed review of sales performance for the fiscal year 2023. \\nThe objective is to analyze revenue trends, regional performance, and category-level \\nbreakdowns. \\nKey goals of this report include: \\n• Identifying revenue growth patterns over time \\n• Comparing regional performance \\n• Highlighting underperforming product categories \\n  '},\n",
       "  {'type': 'page',\n",
       "   'page_index': 1,\n",
       "   'text': 'Quarterly Revenue Overview \\n \\nTotal revenue for 2023 showed a steady upward trend, with notable acceleration in Q3. \\nQuarterly revenue figures (in million USD): \\n• Q1: 12.4 \\n• Q2: 14.1 \\n• Q3: 18.7 \\n• Q4: 19.3 \\nThe growth observed in Q3 coincides with the launch of a new marketing campaign. \\n  '},\n",
       "  {'type': 'page',\n",
       "   'page_index': 2,\n",
       "   'text': 'Regional Performance Analysis \\nSales performance varied significantly by region. \\n• North America remained the strongest region, contributing over 45% of total \\nrevenue. \\n• Europe experienced moderate growth, particularly in Germany and France. \\n• Asia-Pacific showed the highest growth rate but from a smaller base. \\nFuture analysis should explore correlations between regional marketing spend and revenue \\ngrowth. \\n '}],\n",
       " 'text': 'Annual Sales Performance Review — 2023 \\nThis document provides a detailed review of sales performance for the fiscal year 2023. \\nThe objective is to analyze revenue trends, regional performance, and category-level \\nbreakdowns. \\nKey goals of this report include: \\n• Identifying revenue growth patterns over time \\n• Comparing regional performance \\n• Highlighting underperforming product categories \\n  \\n\\nQuarterly Revenue Overview \\n \\nTotal revenue for 2023 showed a steady upward trend, with notable acceleration in Q3. \\nQuarterly revenue figures (in million USD): \\n• Q1: 12.4 \\n• Q2: 14.1 \\n• Q3: 18.7 \\n• Q4: 19.3 \\nThe growth observed in Q3 coincides with the launch of a new marketing campaign. \\n  \\n\\nRegional Performance Analysis \\nSales performance varied significantly by region. \\n• North America remained the strongest region, contributing over 45% of total \\nrevenue. \\n• Europe experienced moderate growth, particularly in Germany and France. \\n• Asia-Pacific showed the highest growth rate but from a smaller base. \\nFuture analysis should explore correlations between regional marketing spend and revenue \\ngrowth. \\n ',\n",
       " 'metadata': {'num_pages': 3}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# takes the first pdf file if there are multiple \n",
    "pdf_file = [f for f in sample_files if Path(f).suffix.lower() == '.pdf'][0]\n",
    "ingest_file(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "430ac027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\islam\\AppData\\Local\\Temp\\ipykernel_39280\\1653241032.py:164: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(sample, errors=\"raise\")\n",
      "C:\\Users\\islam\\AppData\\Local\\Temp\\ipykernel_39280\\1653241032.py:164: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(sample, errors=\"raise\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'source_path': 'data\\\\raw\\\\sample.csv',\n",
       " 'file_type': 'csv',\n",
       " 'blocks': [{'type': 'table',\n",
       "   'rows': [{'date': '2023-01-01',\n",
       "     'region': 'North America',\n",
       "     'product_category': 'Electronics',\n",
       "     'revenue': 1200000,\n",
       "     'units_sold': 3400},\n",
       "    {'date': '2023-02-01',\n",
       "     'region': 'North America',\n",
       "     'product_category': 'Electronics',\n",
       "     'revenue': 1250000,\n",
       "     'units_sold': 3600},\n",
       "    {'date': '2023-03-01',\n",
       "     'region': 'Europe',\n",
       "     'product_category': 'Home Appliances',\n",
       "     'revenue': 980000,\n",
       "     'units_sold': 2100},\n",
       "    {'date': '2023-04-01',\n",
       "     'region': 'Europe',\n",
       "     'product_category': 'Home Appliances',\n",
       "     'revenue': 1020000,\n",
       "     'units_sold': 2300},\n",
       "    {'date': '2023-05-01',\n",
       "     'region': 'Asia-Pacific',\n",
       "     'product_category': 'Accessories',\n",
       "     'revenue': 640000,\n",
       "     'units_sold': 4100},\n",
       "    {'date': '2023-06-01',\n",
       "     'region': 'Asia-Pacific',\n",
       "     'product_category': 'Accessories',\n",
       "     'revenue': 690000,\n",
       "     'units_sold': 4500},\n",
       "    {'date': '2023-07-01',\n",
       "     'region': 'North America',\n",
       "     'product_category': 'Electronics',\n",
       "     'revenue': 1500000,\n",
       "     'units_sold': 4200},\n",
       "    {'date': '2023-08-01',\n",
       "     'region': 'Europe',\n",
       "     'product_category': 'Electronics',\n",
       "     'revenue': 1100000,\n",
       "     'units_sold': 3000},\n",
       "    {'date': '2023-09-01',\n",
       "     'region': 'Asia-Pacific',\n",
       "     'product_category': 'Electronics',\n",
       "     'revenue': 980000,\n",
       "     'units_sold': 2700}]}],\n",
       " 'text': 'date,region,product_category,revenue,units_sold\\r\\n2023-01-01,North America,Electronics,1200000,3400\\r\\n2023-02-01,North America,Electronics,1250000,3600\\r\\n2023-03-01,Europe,Home Appliances,980000,2100\\r\\n2023-04-01,Europe,Home Appliances,1020000,2300\\r\\n2023-05-01,Asia-Pacific,Accessories,640000,4100\\r\\n2023-06-01,Asia-Pacific,Accessories,690000,4500\\r\\n2023-07-01,North America,Electronics,1500000,4200\\r\\n2023-08-01,Europe,Electronics,1100000,3000\\r\\n2023-09-01,Asia-Pacific,Electronics,980000,2700\\r\\n',\n",
       " 'metadata': {'num_rows': 9,\n",
       "  'num_columns': 5,\n",
       "  'schema': {'date': 'datetime',\n",
       "   'region': 'object',\n",
       "   'product_category': 'object',\n",
       "   'revenue': 'int64',\n",
       "   'units_sold': 'int64'}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# takes the first csv file if there are multiple \n",
    "csv_file = [f for f in sample_files if Path(f).suffix.lower() == '.csv'][0]\n",
    "ingest_file(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09f00680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source_path': 'data\\\\raw\\\\sample.docx',\n",
       " 'file_type': 'docx',\n",
       " 'blocks': [{'type': 'heading',\n",
       "   'level': 1,\n",
       "   'text': 'Customer Churn Analysis — Internal Memo'},\n",
       "  {'type': 'heading', 'level': 2, 'text': 'Background'},\n",
       "  {'type': 'paragraph',\n",
       "   'text': 'Customer churn has become a growing concern over the past two years.\\nUnderstanding churn patterns is critical for improving retention strategies.'},\n",
       "  {'type': 'heading', 'level': 2, 'text': 'Data Sources'},\n",
       "  {'type': 'paragraph',\n",
       "   'text': 'The analysis is based on the following sources:'},\n",
       "  {'type': 'paragraph', 'text': 'Monthly customer subscription records'},\n",
       "  {'type': 'paragraph', 'text': 'Support ticket logs'},\n",
       "  {'type': 'paragraph', 'text': 'Customer satisfaction survey results'},\n",
       "  {'type': 'paragraph',\n",
       "   'text': 'Each data source contributes a different perspective on customer behavior.'},\n",
       "  {'type': 'heading', 'level': 2, 'text': 'Key Findings'},\n",
       "  {'type': 'paragraph',\n",
       "   'text': 'The analysis identified several notable patterns:'},\n",
       "  {'type': 'paragraph',\n",
       "   'text': 'Customers with unresolved support tickets churn at a higher rate'},\n",
       "  {'type': 'paragraph',\n",
       "   'text': 'Churn probability increases after three consecutive months of inactivity'},\n",
       "  {'type': 'paragraph',\n",
       "   'text': 'High-value customers are less likely to churn'},\n",
       "  {'type': 'heading', 'level': 2, 'text': 'Recommendations'},\n",
       "  {'type': 'paragraph',\n",
       "   'text': 'Based on the findings, the following actions are recommended:'},\n",
       "  {'type': 'paragraph',\n",
       "   'text': 'Prioritize resolution of long-standing support tickets'},\n",
       "  {'type': 'paragraph',\n",
       "   'text': 'Introduce engagement campaigns for inactive users'},\n",
       "  {'type': 'paragraph',\n",
       "   'text': 'Develop loyalty incentives for high-value customers'}],\n",
       " 'text': 'Customer Churn Analysis — Internal Memo\\nBackground\\nCustomer churn has become a growing concern over the past two years.\\nUnderstanding churn patterns is critical for improving retention strategies.\\nData Sources\\nThe analysis is based on the following sources:\\nMonthly customer subscription records\\nSupport ticket logs\\nCustomer satisfaction survey results\\nEach data source contributes a different perspective on customer behavior.\\nKey Findings\\nThe analysis identified several notable patterns:\\nCustomers with unresolved support tickets churn at a higher rate\\nChurn probability increases after three consecutive months of inactivity\\nHigh-value customers are less likely to churn\\nRecommendations\\nBased on the findings, the following actions are recommended:\\nPrioritize resolution of long-standing support tickets\\nIntroduce engagement campaigns for inactive users\\nDevelop loyalty incentives for high-value customers',\n",
       " 'metadata': {'num_blocks': 19}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# takes the first pdf file if there are multiple \n",
    "docx_file = [f for f in sample_files if Path(f).suffix.lower() == '.docx'][0]\n",
    "ingest_file(docx_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8856a2",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ab8fd6",
   "metadata": {},
   "source": [
    "uncomment below to create chunking.py in core_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5632f2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing core/chunking.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile core/chunking.py\n",
    "\n",
    "\n",
    "\n",
    "import hashlib\n",
    "\n",
    "\n",
    "def make_chunk_id(source_path: str, local_id: str) -> str:\n",
    "    raw = f\"{source_path}::{local_id}\"\n",
    "    return hashlib.sha256(raw.encode(\"utf-8\")).hexdigest()[:16]\n",
    "\n",
    "\n",
    "def chunk_pdf(doc: dict) -> list:\n",
    "    chunks = []\n",
    "\n",
    "    for block in doc[\"blocks\"]:\n",
    "        page_idx = block[\"page_index\"]\n",
    "        text = block[\"text\"].strip()\n",
    "\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        chunk_id = make_chunk_id(\n",
    "            doc[\"source_path\"],\n",
    "            f\"page_{page_idx}\"\n",
    "        )\n",
    "\n",
    "        chunks.append({\n",
    "            \"id\": chunk_id,\n",
    "            \"source\": doc[\"source_path\"],\n",
    "            \"file_type\": \"pdf\",\n",
    "            \"text\": text,\n",
    "            \"metadata\": {\n",
    "                \"page_index\": page_idx,\n",
    "                \"block_type\": block[\"type\"]\n",
    "            }\n",
    "        })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def chunk_docx(doc: dict) -> list:\n",
    "    chunks = []\n",
    "    current_heading = None\n",
    "    buffer = []\n",
    "\n",
    "    for block in doc[\"blocks\"]:\n",
    "        if block[\"type\"] == \"heading\":\n",
    "            if buffer:\n",
    "                chunk_id = make_chunk_id(\n",
    "                    doc[\"source_path\"],\n",
    "                    current_heading or \"intro\"\n",
    "                )\n",
    "\n",
    "                chunks.append({\n",
    "                    \"id\": chunk_id,\n",
    "                    \"source\": doc[\"source_path\"],\n",
    "                    \"file_type\": \"docx\",\n",
    "                    \"text\": \"\\n\".join(buffer),\n",
    "                    \"metadata\": {\n",
    "                        \"section\": current_heading\n",
    "                    }\n",
    "                })\n",
    "\n",
    "                buffer = []\n",
    "\n",
    "            current_heading = block[\"text\"]\n",
    "\n",
    "        elif block[\"type\"] == \"paragraph\":\n",
    "            buffer.append(block[\"text\"])\n",
    "\n",
    "    # flush last section\n",
    "    if buffer:\n",
    "        chunk_id = make_chunk_id(\n",
    "            doc[\"source_path\"],\n",
    "            current_heading or \"final\"\n",
    "        )\n",
    "\n",
    "        chunks.append({\n",
    "            \"id\": chunk_id,\n",
    "            \"source\": doc[\"source_path\"],\n",
    "            \"file_type\": \"docx\",\n",
    "            \"text\": \"\\n\".join(buffer),\n",
    "            \"metadata\": {\n",
    "                \"section\": current_heading\n",
    "            }\n",
    "        })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def chunk_pptx(doc: dict) -> list:\n",
    "    chunks = []\n",
    "\n",
    "    for slide in doc[\"blocks\"]:\n",
    "        slide_idx = slide[\"slide_index\"]\n",
    "\n",
    "        texts = []\n",
    "        title = None\n",
    "\n",
    "        for el in slide[\"elements\"]:\n",
    "            if el[\"role\"] == \"title\":\n",
    "                title = el[\"text\"]\n",
    "            texts.append(el[\"text\"])\n",
    "\n",
    "        content = \"\\n\".join(texts).strip()\n",
    "        if not content:\n",
    "            continue\n",
    "\n",
    "        chunk_id = make_chunk_id(\n",
    "            doc[\"source_path\"],\n",
    "            f\"slide_{slide_idx}\"\n",
    "        )\n",
    "\n",
    "        chunks.append({\n",
    "            \"id\": chunk_id,\n",
    "            \"source\": doc[\"source_path\"],\n",
    "            \"file_type\": \"pptx\",\n",
    "            \"text\": content,\n",
    "            \"metadata\": {\n",
    "                \"slide_index\": slide_idx,\n",
    "                \"title\": title\n",
    "            }\n",
    "        })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def chunk_csv(doc: dict) -> list:\n",
    "    chunk_id = make_chunk_id(\n",
    "        doc[\"source_path\"],\n",
    "        \"table\"\n",
    "    )\n",
    "\n",
    "    return [{\n",
    "        \"id\": chunk_id,\n",
    "        \"source\": doc[\"source_path\"],\n",
    "        \"file_type\": \"csv\",\n",
    "        \"text\": doc[\"text\"],\n",
    "        \"metadata\": {\n",
    "            \"schema\": doc[\"metadata\"][\"schema\"],\n",
    "            \"num_rows\": doc[\"metadata\"][\"num_rows\"]\n",
    "        }\n",
    "    }]\n",
    "\n",
    "\n",
    "def chunk_txt(doc: dict, max_chars: int = 800, overlap: int = 100) -> list:\n",
    "    text = doc[\"text\"]\n",
    "    chunks = []\n",
    "\n",
    "    start = 0\n",
    "    idx = 0\n",
    "\n",
    "    while start < len(text):\n",
    "        end = start + max_chars\n",
    "        chunk_text = text[start:end].strip()\n",
    "\n",
    "        if not chunk_text:\n",
    "            break\n",
    "\n",
    "        chunk_id = make_chunk_id(\n",
    "            doc[\"source_path\"],\n",
    "            f\"chunk_{idx}\"\n",
    "        )\n",
    "\n",
    "        chunks.append({\n",
    "            \"id\": chunk_id,\n",
    "            \"source\": doc[\"source_path\"],\n",
    "            \"file_type\": \"txt\",\n",
    "            \"text\": chunk_text,\n",
    "            \"metadata\": {\n",
    "                \"char_start\": start,\n",
    "                \"char_end\": min(end, len(text))\n",
    "            }\n",
    "        })\n",
    "\n",
    "        start = end - overlap\n",
    "        idx += 1\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def chunk_xlsx(doc: dict) -> list:\n",
    "    chunks = []\n",
    "\n",
    "    for block in doc[\"blocks\"]:\n",
    "        sheet_name = block[\"sheet_name\"]\n",
    "        df = block[\"dataframe\"]\n",
    "\n",
    "        chunk_id = make_chunk_id(\n",
    "            doc[\"source_path\"],\n",
    "            f\"sheet_{sheet_name}\"\n",
    "        )\n",
    "\n",
    "        chunks.append({\n",
    "            \"id\": chunk_id,\n",
    "            \"source\": doc[\"source_path\"],\n",
    "            \"file_type\": \"xlsx\",\n",
    "            \"text\": df.to_csv(index=False),\n",
    "            \"metadata\": {\n",
    "                \"sheet_name\": sheet_name,\n",
    "                \"num_rows\": df.shape[0],\n",
    "                \"num_columns\": df.shape[1],\n",
    "                \"columns\": list(df.columns)\n",
    "            }\n",
    "        })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def chunk_document(doc: dict) -> list:\n",
    "    file_type = doc[\"file_type\"]\n",
    "\n",
    "    if file_type == \"pdf\":\n",
    "        return chunk_pdf(doc)\n",
    "    elif file_type == \"docx\":\n",
    "        return chunk_docx(doc)\n",
    "    elif file_type == \"pptx\":\n",
    "        return chunk_pptx(doc)\n",
    "    elif file_type == \"csv\":\n",
    "        return chunk_csv(doc)\n",
    "    elif file_type =='txt':\n",
    "        return chunk_txt(doc)\n",
    "    elif file_type == 'xlsx':\n",
    "        return chunk_xlsx(doc)\n",
    "    else:\n",
    "        raise ValueError(f\"No chunker for {file_type}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d1dcc3",
   "metadata": {},
   "source": [
    "Demo of ingestion + chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca6d477a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Metadata: {'section': 'Background'}\n",
      "Text preview:\n",
      "Customer churn has become a growing concern over the past two years.\n",
      "Understanding churn patterns is\n",
      "============================================================\n",
      "Metadata: {'section': 'Data Sources'}\n",
      "Text preview:\n",
      "The analysis is based on the following sources:\n",
      "Monthly customer subscription records\n",
      "Support ticket\n",
      "============================================================\n",
      "Metadata: {'section': 'Key Findings'}\n",
      "Text preview:\n",
      "The analysis identified several notable patterns:\n",
      "Customers with unresolved support tickets churn at\n",
      "============================================================\n",
      "Metadata: {'section': 'Recommendations'}\n",
      "Text preview:\n",
      "Based on the findings, the following actions are recommended:\n",
      "Prioritize resolution of long-standing\n"
     ]
    }
   ],
   "source": [
    "doc = ingest_file(\"data/raw/sample.docx\")\n",
    "chunks = chunk_document(doc)\n",
    "\n",
    "for c in chunks:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Metadata:\", c[\"metadata\"])\n",
    "    print(\"Text preview:\")\n",
    "    print(c[\"text\"][:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32f1ce2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "{'slide_index': 0, 'title': None}\n",
      "Text preview:\n",
      "2023 Product Performance Summary\n",
      "Overview of key performance indicators\n",
      "Focus on revenue and custome\n",
      "============================================================\n",
      "{'slide_index': 1, 'title': 'Top Performing Products'}\n",
      "Text preview:\n",
      "Top Performing Products\n",
      "Product A showed consistent growth\n",
      "Product B peaked in mid-year\n",
      "Product C un\n",
      "============================================================\n",
      "{'slide_index': 2, 'title': 'Revenue by Product Category'}\n",
      "Text preview:\n",
      "Revenue by Product Category\n",
      "============================================================\n",
      "{'slide_index': 3, 'title': 'Quarterly Trends'}\n",
      "Text preview:\n",
      "Quarterly Trends\n",
      "Q1 and Q2 growth was moderate\n",
      "Q3 saw a sharp increase\n",
      "Q4 stabilized at a higher bas\n",
      "============================================================\n",
      "{'slide_index': 4, 'title': 'Key Takeaways'}\n",
      "Text preview:\n",
      "Key Takeaways\n",
      "Electronics remain the primary revenue driver\n",
      "Accessories require strategic reevaluati\n"
     ]
    }
   ],
   "source": [
    "doc = ingest_file(\"data/raw/sample.pptx\")\n",
    "chunks = chunk_document(doc)\n",
    "\n",
    "for c in chunks:\n",
    "    print(\"=\" * 60)\n",
    "    print(c[\"metadata\"])\n",
    "    print(\"Text preview:\")\n",
    "    print(c[\"text\"][:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67189ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "{'page_index': 0, 'block_type': 'page'}\n",
      "Annual Sales Performance Review — 2023 \n",
      "This document provides a detailed review of sales performanc\n",
      "============================================================\n",
      "{'page_index': 1, 'block_type': 'page'}\n",
      "Quarterly Revenue Overview \n",
      " \n",
      "Total revenue for 2023 showed a steady upward trend, with notable acce\n",
      "============================================================\n",
      "{'page_index': 2, 'block_type': 'page'}\n",
      "Regional Performance Analysis \n",
      "Sales performance varied significantly by region. \n",
      "• North America re\n"
     ]
    }
   ],
   "source": [
    "doc = ingest_file(\"data/raw/sample.pdf\")\n",
    "chunks = chunk_document(doc)\n",
    "\n",
    "for c in chunks:\n",
    "    print(\"=\" * 60)\n",
    "    print(c[\"metadata\"])\n",
    "    print(c[\"text\"][:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c3c6dd",
   "metadata": {},
   "source": [
    "## Simple Information Retrieval using lexical socre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3477daa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import re\n",
    "\n",
    "def search_chunks(query: str, chunks: List[dict], top_k: int = 5) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Simple lexical search: count query occurrence in each chunk\n",
    "    Returns top_k chunks sorted by count descending\n",
    "    \"\"\"\n",
    "    query_lower = query.lower()\n",
    "    scored = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        text_lower = chunk[\"text\"].lower()\n",
    "        score = text_lower.count(query_lower)\n",
    "\n",
    "        if score > 0:\n",
    "            scored.append((score, chunk))\n",
    "\n",
    "    # Sort descending by score\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    return [c for _, c in scored[:top_k]]\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def build_corpus(file_paths: list[str]) -> list[dict]:\n",
    "    corpus = []\n",
    "\n",
    "    for path in file_paths:\n",
    "        doc = ingest_file(path)\n",
    "        chunks = chunk_document(doc)\n",
    "        corpus.extend(chunks)\n",
    "\n",
    "    return corpus\n",
    "\n",
    "def lexical_score(text: str, query: str) -> int:\n",
    "    tokens = re.findall(r\"\\w+\", query.lower())\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    return sum(text_lower.count(tok) for tok in tokens)\n",
    "\n",
    "def search_corpus(query: str, corpus: list[dict], top_k: int = 5) -> list[dict]:\n",
    "    scored = []\n",
    "\n",
    "    for chunk in corpus:\n",
    "        score = lexical_score(chunk[\"text\"], query)\n",
    "        if score > 0:\n",
    "            scored.append((score, chunk))\n",
    "\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "    return [c for _, c in scored[:top_k]]\n",
    "\n",
    "def display_results(results: list[dict]):\n",
    "    for rank, chunk in enumerate(results, start=1):\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Rank: {rank}\")\n",
    "        print(\"Source:\", chunk[\"source\"])\n",
    "        print(\"File type:\", chunk[\"file_type\"])\n",
    "        print(\"Metadata:\", chunk[\"metadata\"])\n",
    "        print(\"Text preview:\")\n",
    "        print(chunk[\"text\"][:400])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85600569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Chunk ID: a45d9ceede09fd42\n",
      "Metadata: {'page_index': 1, 'block_type': 'page'}\n",
      "Text snippet: Quarterly Revenue Overview \n",
      " \n",
      "Total revenue for 2023 showed a steady upward trend, with notable acceleration in Q3. \n",
      "Quarterly revenue figures (in million USD): \n",
      "• Q1: 12.4 \n",
      "• Q2: 14.1 \n",
      "• Q3: 18.7 \n",
      "• Q4: 19.3 \n",
      "The growth observed in Q3 coincides with the launch of a new marketing campaign.\n",
      "============================================================\n",
      "Chunk ID: 6dbe80d88422232e\n",
      "Metadata: {'page_index': 0, 'block_type': 'page'}\n",
      "Text snippet: Annual Sales Performance Review — 2023 \n",
      "This document provides a detailed review of sales performance for the fiscal year 2023. \n",
      "The objective is to analyze revenue trends, regional performance, and category-level \n",
      "breakdowns. \n",
      "Key goals of this report include: \n",
      "• Identifying revenue growth patterns\n",
      "============================================================\n",
      "Chunk ID: a95f2e24ce11c218\n",
      "Metadata: {'page_index': 2, 'block_type': 'page'}\n",
      "Text snippet: Regional Performance Analysis \n",
      "Sales performance varied significantly by region. \n",
      "• North America remained the strongest region, contributing over 45% of total \n",
      "revenue. \n",
      "• Europe experienced moderate growth, particularly in Germany and France. \n",
      "• Asia-Pacific showed the highest growth rate but from\n"
     ]
    }
   ],
   "source": [
    "doc = ingest_file(\"data/raw/sample.pdf\")\n",
    "chunks = chunk_document(doc)\n",
    "\n",
    "query = \"revenue\"\n",
    "results = search_chunks(query, chunks, top_k=3)\n",
    "\n",
    "for r in results:\n",
    "    print(\"=\"*60)\n",
    "    print(\"Chunk ID:\", r[\"id\"])\n",
    "    print(\"Metadata:\", r[\"metadata\"])\n",
    "    print(\"Text snippet:\", r[\"text\"][:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30859f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks indexed: 13\n",
      "================================================================================\n",
      "Rank: 1\n",
      "Source: data\\raw\\sample.pdf\n",
      "File type: pdf\n",
      "Metadata: {'page_index': 2, 'block_type': 'page'}\n",
      "Text preview:\n",
      "Regional Performance Analysis \n",
      "Sales performance varied significantly by region. \n",
      "• North America remained the strongest region, contributing over 45% of total \n",
      "revenue. \n",
      "• Europe experienced moderate growth, particularly in Germany and France. \n",
      "• Asia-Pacific showed the highest growth rate but from a smaller base. \n",
      "Future analysis should explore correlations between regional marketing spend and r\n",
      "================================================================================\n",
      "Rank: 2\n",
      "Source: data\\raw\\sample.pdf\n",
      "File type: pdf\n",
      "Metadata: {'page_index': 0, 'block_type': 'page'}\n",
      "Text preview:\n",
      "Annual Sales Performance Review — 2023 \n",
      "This document provides a detailed review of sales performance for the fiscal year 2023. \n",
      "The objective is to analyze revenue trends, regional performance, and category-level \n",
      "breakdowns. \n",
      "Key goals of this report include: \n",
      "• Identifying revenue growth patterns over time \n",
      "• Comparing regional performance \n",
      "• Highlighting underperforming product categories\n",
      "================================================================================\n",
      "Rank: 3\n",
      "Source: data\\raw\\sample.pdf\n",
      "File type: pdf\n",
      "Metadata: {'page_index': 1, 'block_type': 'page'}\n",
      "Text preview:\n",
      "Quarterly Revenue Overview \n",
      " \n",
      "Total revenue for 2023 showed a steady upward trend, with notable acceleration in Q3. \n",
      "Quarterly revenue figures (in million USD): \n",
      "• Q1: 12.4 \n",
      "• Q2: 14.1 \n",
      "• Q3: 18.7 \n",
      "• Q4: 19.3 \n",
      "The growth observed in Q3 coincides with the launch of a new marketing campaign.\n",
      "================================================================================\n",
      "Rank: 4\n",
      "Source: data\\raw\\sample.pptx\n",
      "File type: pptx\n",
      "Metadata: {'slide_index': 2, 'title': 'Revenue by Product Category'}\n",
      "Text preview:\n",
      "Revenue by Product Category\n",
      "================================================================================\n",
      "Rank: 5\n",
      "Source: data\\raw\\sample.csv\n",
      "File type: csv\n",
      "Metadata: {'schema': {'date': 'datetime', 'region': 'object', 'product_category': 'object', 'revenue': 'int64', 'units_sold': 'int64'}, 'num_rows': 9}\n",
      "Text preview:\n",
      "date,region,product_category,revenue,units_sold\n",
      "2023-01-01,North America,Electronics,1200000,3400\n",
      "2023-02-01,North America,Electronics,1250000,3600\n",
      "2023-03-01,Europe,Home Appliances,980000,2100\n",
      "2023-04-01,Europe,Home Appliances,1020000,2300\n",
      "2023-05-01,Asia-Pacific,Accessories,640000,4100\n",
      "2023-06-01,Asia-Pacific,Accessories,690000,4500\n",
      "2023-07-01,North America,Electronics,1500000,4200\n",
      "2023-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\islam\\AppData\\Local\\Temp\\ipykernel_39280\\1653241032.py:164: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(sample, errors=\"raise\")\n",
      "C:\\Users\\islam\\AppData\\Local\\Temp\\ipykernel_39280\\1653241032.py:164: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(sample, errors=\"raise\")\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "        \"data/raw/sample.pdf\",\n",
    "        \"data/raw/sample.docx\",\n",
    "        \"data/raw/sample.pptx\",\n",
    "        \"data/raw/sample.csv\",\n",
    "        \n",
    "    ]\n",
    "\n",
    "corpus = build_corpus(files)\n",
    "print(f\"Total chunks indexed: {len(corpus)}\")\n",
    "\n",
    "query = \"revenue growth by region\"\n",
    "results = search_corpus(query, corpus, top_k=5)\n",
    "\n",
    "display_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509582a8",
   "metadata": {},
   "source": [
    "## Semantic Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee519fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def embed_chunks(chunks: list[dict]) -> np.ndarray:\n",
    "    texts = [c[\"text\"] for c in chunks]\n",
    "    embeddings = model.encode(texts, show_progress_bar=True)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return (a @ b) / (norm(a) * norm(b))\n",
    "\n",
    "def semantic_search(query: str, chunks: list[dict], embeddings: np.ndarray, top_k: int = 5):\n",
    "    query_emb = model.encode(query)\n",
    "\n",
    "    scores = []\n",
    "    for emb, chunk in zip(embeddings, chunks):\n",
    "        score = cosine_similarity(query_emb, emb)\n",
    "        scores.append((score, chunk))\n",
    "\n",
    "    scores.sort(key=lambda x: x[0], reverse=True)\n",
    "    return [c for _, c in scores[:top_k]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b63388c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\islam\\AppData\\Local\\Temp\\ipykernel_39280\\1653241032.py:164: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(sample, errors=\"raise\")\n",
      "C:\\Users\\islam\\AppData\\Local\\Temp\\ipykernel_39280\\1653241032.py:164: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(sample, errors=\"raise\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c2c4d587e74702adb4d17db0e93487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Rank: 1\n",
      "Source: data\\raw\\sample.pptx\n",
      "File type: pptx\n",
      "Metadata: {'slide_index': 1, 'title': 'Top Performing Products'}\n",
      "Text preview:\n",
      "Top Performing Products\n",
      "Product A showed consistent growth\n",
      "Product B peaked in mid-year\n",
      "Product C underperformed expectations\n",
      "================================================================================\n",
      "Rank: 2\n",
      "Source: data\\raw\\sample.pptx\n",
      "File type: pptx\n",
      "Metadata: {'slide_index': 0, 'title': None}\n",
      "Text preview:\n",
      "2023 Product Performance Summary\n",
      "Overview of key performance indicators\n",
      "Focus on revenue and customer adoption\n",
      "Comparison across product categories\n",
      "================================================================================\n",
      "Rank: 3\n",
      "Source: data\\raw\\sample.pdf\n",
      "File type: pdf\n",
      "Metadata: {'page_index': 0, 'block_type': 'page'}\n",
      "Text preview:\n",
      "Annual Sales Performance Review — 2023 \n",
      "This document provides a detailed review of sales performance for the fiscal year 2023. \n",
      "The objective is to analyze revenue trends, regional performance, and category-level \n",
      "breakdowns. \n",
      "Key goals of this report include: \n",
      "• Identifying revenue growth patterns over time \n",
      "• Comparing regional performance \n",
      "• Highlighting underperforming product categories\n",
      "================================================================================\n",
      "Rank: 4\n",
      "Source: data\\raw\\sample.pptx\n",
      "File type: pptx\n",
      "Metadata: {'slide_index': 4, 'title': 'Key Takeaways'}\n",
      "Text preview:\n",
      "Key Takeaways\n",
      "Electronics remain the primary revenue driver\n",
      "Accessories require strategic reevaluation\n",
      "Continued investment in product innovation is recommended\n",
      "================================================================================\n",
      "Rank: 5\n",
      "Source: data\\raw\\sample.csv\n",
      "File type: csv\n",
      "Metadata: {'schema': {'date': 'datetime', 'region': 'object', 'product_category': 'object', 'revenue': 'int64', 'units_sold': 'int64'}, 'num_rows': 9}\n",
      "Text preview:\n",
      "date,region,product_category,revenue,units_sold\n",
      "2023-01-01,North America,Electronics,1200000,3400\n",
      "2023-02-01,North America,Electronics,1250000,3600\n",
      "2023-03-01,Europe,Home Appliances,980000,2100\n",
      "2023-04-01,Europe,Home Appliances,1020000,2300\n",
      "2023-05-01,Asia-Pacific,Accessories,640000,4100\n",
      "2023-06-01,Asia-Pacific,Accessories,690000,4500\n",
      "2023-07-01,North America,Electronics,1500000,4200\n",
      "2023-\n"
     ]
    }
   ],
   "source": [
    "corpus = build_corpus(files)\n",
    "embeddings = embed_chunks(corpus)\n",
    "\n",
    "query = \"Which products underperformed in 2023?\"\n",
    "results = semantic_search(query, corpus, embeddings, top_k=5)\n",
    "\n",
    "display_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf466b5",
   "metadata": {},
   "source": [
    "## Hybrid Retrieval\n",
    "\n",
    "Hybrid scoring function\n",
    "Design choice (important)\n",
    "\n",
    "- Default weights:\n",
    "\n",
    "    - α = 0.6 semantic\n",
    "\n",
    "    - β = 0.4 lexical\n",
    "\n",
    "Semantic slightly dominates, but lexical can *override* nonsense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71d7fb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(scores: list[float]) -> list[float]:\n",
    "    if not scores:\n",
    "        return scores\n",
    "\n",
    "    min_s, max_s = min(scores), max(scores)\n",
    "    if min_s == max_s:\n",
    "        return [1.0] * len(scores)\n",
    "\n",
    "    return [(s - min_s) / (max_s - min_s) for s in scores]\n",
    "\n",
    "\n",
    "def hybrid_search(\n",
    "    query: str,\n",
    "    chunks: list[dict],\n",
    "    embeddings,\n",
    "    alpha: float = 0.6,\n",
    "    beta: float = 0.4,\n",
    "    top_k: int = 5\n",
    "):\n",
    "    # --- Lexical scores ---\n",
    "    lexical_scores = [\n",
    "        lexical_score(chunk[\"text\"], query)\n",
    "        for chunk in chunks\n",
    "    ]\n",
    "    lexical_norm = min_max_normalize(lexical_scores)\n",
    "\n",
    "    # --- Semantic scores ---\n",
    "    query_emb = model.encode(query)\n",
    "    semantic_scores = [\n",
    "        cosine_similarity(query_emb, emb)\n",
    "        for emb in embeddings\n",
    "    ]\n",
    "    semantic_norm = min_max_normalize(semantic_scores)\n",
    "\n",
    "    # --- Combine ---\n",
    "    combined = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        score = alpha * semantic_norm[i] + beta * lexical_norm[i]\n",
    "        combined.append((score, chunk))\n",
    "\n",
    "    combined.sort(key=lambda x: x[0], reverse=True)\n",
    "    return [c for _, c in combined[:top_k]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bfe2046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\islam\\AppData\\Local\\Temp\\ipykernel_39280\\1653241032.py:164: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(sample, errors=\"raise\")\n",
      "C:\\Users\\islam\\AppData\\Local\\Temp\\ipykernel_39280\\1653241032.py:164: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(sample, errors=\"raise\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adcc1805359d476ebc278efb8bd73e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Rank: 1\n",
      "Source: data\\raw\\sample.pdf\n",
      "File type: pdf\n",
      "Metadata: {'page_index': 2, 'block_type': 'page'}\n",
      "Text preview:\n",
      "Regional Performance Analysis \n",
      "Sales performance varied significantly by region. \n",
      "• North America remained the strongest region, contributing over 45% of total \n",
      "revenue. \n",
      "• Europe experienced moderate growth, particularly in Germany and France. \n",
      "• Asia-Pacific showed the highest growth rate but from a smaller base. \n",
      "Future analysis should explore correlations between regional marketing spend and r\n",
      "================================================================================\n",
      "Rank: 2\n",
      "Source: data\\raw\\sample.pdf\n",
      "File type: pdf\n",
      "Metadata: {'page_index': 0, 'block_type': 'page'}\n",
      "Text preview:\n",
      "Annual Sales Performance Review — 2023 \n",
      "This document provides a detailed review of sales performance for the fiscal year 2023. \n",
      "The objective is to analyze revenue trends, regional performance, and category-level \n",
      "breakdowns. \n",
      "Key goals of this report include: \n",
      "• Identifying revenue growth patterns over time \n",
      "• Comparing regional performance \n",
      "• Highlighting underperforming product categories\n",
      "================================================================================\n",
      "Rank: 3\n",
      "Source: data\\raw\\sample.pdf\n",
      "File type: pdf\n",
      "Metadata: {'page_index': 1, 'block_type': 'page'}\n",
      "Text preview:\n",
      "Quarterly Revenue Overview \n",
      " \n",
      "Total revenue for 2023 showed a steady upward trend, with notable acceleration in Q3. \n",
      "Quarterly revenue figures (in million USD): \n",
      "• Q1: 12.4 \n",
      "• Q2: 14.1 \n",
      "• Q3: 18.7 \n",
      "• Q4: 19.3 \n",
      "The growth observed in Q3 coincides with the launch of a new marketing campaign.\n",
      "================================================================================\n",
      "Rank: 4\n",
      "Source: data\\raw\\sample.pptx\n",
      "File type: pptx\n",
      "Metadata: {'slide_index': 3, 'title': 'Quarterly Trends'}\n",
      "Text preview:\n",
      "Quarterly Trends\n",
      "Q1 and Q2 growth was moderate\n",
      "Q3 saw a sharp increase\n",
      "Q4 stabilized at a higher baseline\n",
      "================================================================================\n",
      "Rank: 5\n",
      "Source: data\\raw\\sample.pptx\n",
      "File type: pptx\n",
      "Metadata: {'slide_index': 2, 'title': 'Revenue by Product Category'}\n",
      "Text preview:\n",
      "Revenue by Product Category\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "        \"data/raw/sample.pdf\",\n",
    "        \"data/raw/sample.docx\",\n",
    "        \"data/raw/sample.pptx\",\n",
    "        \"data/raw/sample.csv\",\n",
    "        \n",
    "    ]\n",
    "\n",
    "corpus = build_corpus(files)\n",
    "embeddings = embed_chunks(corpus)\n",
    "\n",
    "query = \"Which regions showed declining revenue trends?\"\n",
    "results = hybrid_search(query, corpus, embeddings, top_k=5)\n",
    "\n",
    "display_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171db1ce",
   "metadata": {},
   "source": [
    "uncomment below to create retrieval.py in core_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a218bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing core/retrieval.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile  core/retrieval.py\n",
    "\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Models\n",
    "# -----------------------------\n",
    "\n",
    "_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return (a @ b) / (norm(a) * norm(b))\n",
    "\n",
    "\n",
    "def lexical_score(text: str, query: str) -> float:\n",
    "    \"\"\"\n",
    "    Simple term overlap score.\n",
    "    Intentionally naive but deterministic.\n",
    "    \"\"\"\n",
    "    text_tokens = set(text.lower().split())\n",
    "    query_tokens = set(query.lower().split())\n",
    "    return len(text_tokens & query_tokens)\n",
    "\n",
    "\n",
    "def min_max_normalize(scores: List[float]) -> List[float]:\n",
    "    if not scores:\n",
    "        return scores\n",
    "\n",
    "    min_s, max_s = min(scores), max(scores)\n",
    "    if min_s == max_s:\n",
    "        return [1.0] * len(scores)\n",
    "\n",
    "    return [(s - min_s) / (max_s - min_s) for s in scores]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Embeddings\n",
    "# -----------------------------\n",
    "\n",
    "def embed_chunks(chunks: List[Dict]) -> np.ndarray:\n",
    "    texts = [c[\"text\"] for c in chunks]\n",
    "    return _model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Simple Retrieval\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "\n",
    "def search_corpus(query: str, corpus: list[dict], top_k: int = 5) -> list[dict]:\n",
    "    scored = []\n",
    "\n",
    "    for chunk in corpus:\n",
    "        score = lexical_score(chunk[\"text\"], query)\n",
    "        if score > 0:\n",
    "            scored.append((score, chunk))\n",
    "\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "    return [c for _, c in scored[:top_k]]\n",
    "\n",
    "# -----------------------------\n",
    "# Semantic Retrieval\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "def semantic_search(query: str, chunks: list[dict], embeddings: np.ndarray, top_k: int = 5):\n",
    "    query_emb = model.encode(query)\n",
    "\n",
    "    scores = []\n",
    "    for emb, chunk in zip(embeddings, chunks):\n",
    "        score = cosine_similarity(query_emb, emb)\n",
    "        scores.append((score, chunk))\n",
    "\n",
    "    scores.sort(key=lambda x: x[0], reverse=True)\n",
    "    return [c for _, c in scores[:top_k]]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Hybrid Retrieval\n",
    "# -----------------------------\n",
    "\n",
    "def hybrid_search(\n",
    "    query: str,\n",
    "    chunks: List[Dict],\n",
    "    embeddings: np.ndarray,\n",
    "    alpha: float = 0.6,\n",
    "    beta: float = 0.4,\n",
    "    top_k: int = 5\n",
    "):\n",
    "    # Lexical\n",
    "    lexical_scores = [lexical_score(c[\"text\"], query) for c in chunks]\n",
    "    lexical_norm = min_max_normalize(lexical_scores)\n",
    "\n",
    "    # Semantic\n",
    "    query_emb = _model.encode(query)\n",
    "    semantic_scores = [\n",
    "        cosine_similarity(query_emb, emb)\n",
    "        for emb in embeddings\n",
    "    ]\n",
    "    semantic_norm = min_max_normalize(semantic_scores)\n",
    "\n",
    "    # Combine\n",
    "    combined = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        score = alpha * semantic_norm[i] + beta * lexical_norm[i]\n",
    "        combined.append((score, chunk))\n",
    "\n",
    "    combined.sort(key=lambda x: x[0], reverse=True)\n",
    "    return [c for _, c in combined[:top_k]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fce0cf",
   "metadata": {},
   "source": [
    "# Putting it altogether, a simple IR system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe29ce9",
   "metadata": {},
   "source": [
    "### Building a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8044ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks in corpus: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\islam\\AppData\\Local\\Temp\\ipykernel_39280\\1653241032.py:164: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(sample, errors=\"raise\")\n",
      "C:\\Users\\islam\\AppData\\Local\\Temp\\ipykernel_39280\\1653241032.py:164: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(sample, errors=\"raise\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': '6dbe80d88422232e',\n",
       " 'source': 'data\\\\raw\\\\sample.pdf',\n",
       " 'file_type': 'pdf',\n",
       " 'text': 'Annual Sales Performance Review — 2023 \\nThis document provides a detailed review of sales performance for the fiscal year 2023. \\nThe objective is to analyze revenue trends, regional performance, and category-level \\nbreakdowns. \\nKey goals of this report include: \\n• Identifying revenue growth patterns over time \\n• Comparing regional performance \\n• Highlighting underperforming product categories',\n",
       " 'metadata': {'page_index': 0, 'block_type': 'page'}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths = [\n",
    "    \"data/raw/sample.pdf\",\n",
    "    \"data/raw/sample.docx\",\n",
    "    \"data/raw/sample.pptx\",\n",
    "    \"data/raw/sample.csv\",\n",
    "    \n",
    "]\n",
    "\n",
    "corpus = build_corpus(file_paths)\n",
    "\n",
    "print(f\"Total chunks in corpus: {len(corpus)}\")\n",
    "\n",
    "corpus[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb47a21",
   "metadata": {},
   "source": [
    "### Lexical Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "baab1b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEXICAL RESULTS\n",
      "================================================================================\n",
      "Rank: 1\n",
      "Source: data\\raw\\sample.pdf\n",
      "File type: pdf\n",
      "Metadata: {'page_index': 2, 'block_type': 'page'}\n",
      "Text preview:\n",
      "Regional Performance Analysis \n",
      "Sales performance varied significantly by region. \n",
      "• North America remained the strongest region, contributing over 45% of total \n",
      "revenue. \n",
      "• Europe experienced moderate growth, particularly in Germany and France. \n",
      "• Asia-Pacific showed the highest growth rate but from a smaller base. \n",
      "Future analysis should explore correlations between regional marketing spend and r\n",
      "================================================================================\n",
      "Rank: 2\n",
      "Source: data\\raw\\sample.pdf\n",
      "File type: pdf\n",
      "Metadata: {'page_index': 1, 'block_type': 'page'}\n",
      "Text preview:\n",
      "Quarterly Revenue Overview \n",
      " \n",
      "Total revenue for 2023 showed a steady upward trend, with notable acceleration in Q3. \n",
      "Quarterly revenue figures (in million USD): \n",
      "• Q1: 12.4 \n",
      "• Q2: 14.1 \n",
      "• Q3: 18.7 \n",
      "• Q4: 19.3 \n",
      "The growth observed in Q3 coincides with the launch of a new marketing campaign.\n",
      "================================================================================\n",
      "Rank: 3\n",
      "Source: data\\raw\\sample.pdf\n",
      "File type: pdf\n",
      "Metadata: {'page_index': 0, 'block_type': 'page'}\n",
      "Text preview:\n",
      "Annual Sales Performance Review — 2023 \n",
      "This document provides a detailed review of sales performance for the fiscal year 2023. \n",
      "The objective is to analyze revenue trends, regional performance, and category-level \n",
      "breakdowns. \n",
      "Key goals of this report include: \n",
      "• Identifying revenue growth patterns over time \n",
      "• Comparing regional performance \n",
      "• Highlighting underperforming product categories\n",
      "================================================================================\n",
      "Rank: 4\n",
      "Source: data\\raw\\sample.pptx\n",
      "File type: pptx\n",
      "Metadata: {'slide_index': 0, 'title': None}\n",
      "Text preview:\n",
      "2023 Product Performance Summary\n",
      "Overview of key performance indicators\n",
      "Focus on revenue and customer adoption\n",
      "Comparison across product categories\n",
      "================================================================================\n",
      "Rank: 5\n",
      "Source: data\\raw\\sample.pptx\n",
      "File type: pptx\n",
      "Metadata: {'slide_index': 1, 'title': 'Top Performing Products'}\n",
      "Text preview:\n",
      "Top Performing Products\n",
      "Product A showed consistent growth\n",
      "Product B peaked in mid-year\n",
      "Product C underperformed expectations\n"
     ]
    }
   ],
   "source": [
    "query = \"revenue growth\"\n",
    "\n",
    "lexical_results = search_corpus(\n",
    "    query=query,\n",
    "    corpus=corpus,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "print(\"LEXICAL RESULTS\")\n",
    "display_results(lexical_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0f3634",
   "metadata": {},
   "source": [
    "### Semantic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc9788e",
   "metadata": {},
   "source": [
    "Embed corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "847be882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b62cd42309b54096b2d70015dd4194e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (13, 384)\n"
     ]
    }
   ],
   "source": [
    "embeddings = embed_chunks(corpus)\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "145c4640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEMANTIC RESULTS\n",
      "================================================================================\n",
      "Rank: 1\n",
      "Source: data\\raw\\sample.pdf\n",
      "File type: pdf\n",
      "Metadata: {'page_index': 1, 'block_type': 'page'}\n",
      "Text preview:\n",
      "Quarterly Revenue Overview \n",
      " \n",
      "Total revenue for 2023 showed a steady upward trend, with notable acceleration in Q3. \n",
      "Quarterly revenue figures (in million USD): \n",
      "• Q1: 12.4 \n",
      "• Q2: 14.1 \n",
      "• Q3: 18.7 \n",
      "• Q4: 19.3 \n",
      "The growth observed in Q3 coincides with the launch of a new marketing campaign.\n",
      "================================================================================\n",
      "Rank: 2\n",
      "Source: data\\raw\\sample.pptx\n",
      "File type: pptx\n",
      "Metadata: {'slide_index': 2, 'title': 'Revenue by Product Category'}\n",
      "Text preview:\n",
      "Revenue by Product Category\n",
      "================================================================================\n",
      "Rank: 3\n",
      "Source: data\\raw\\sample.pdf\n",
      "File type: pdf\n",
      "Metadata: {'page_index': 0, 'block_type': 'page'}\n",
      "Text preview:\n",
      "Annual Sales Performance Review — 2023 \n",
      "This document provides a detailed review of sales performance for the fiscal year 2023. \n",
      "The objective is to analyze revenue trends, regional performance, and category-level \n",
      "breakdowns. \n",
      "Key goals of this report include: \n",
      "• Identifying revenue growth patterns over time \n",
      "• Comparing regional performance \n",
      "• Highlighting underperforming product categories\n",
      "================================================================================\n",
      "Rank: 4\n",
      "Source: data\\raw\\sample.pptx\n",
      "File type: pptx\n",
      "Metadata: {'slide_index': 1, 'title': 'Top Performing Products'}\n",
      "Text preview:\n",
      "Top Performing Products\n",
      "Product A showed consistent growth\n",
      "Product B peaked in mid-year\n",
      "Product C underperformed expectations\n",
      "================================================================================\n",
      "Rank: 5\n",
      "Source: data\\raw\\sample.pdf\n",
      "File type: pdf\n",
      "Metadata: {'page_index': 2, 'block_type': 'page'}\n",
      "Text preview:\n",
      "Regional Performance Analysis \n",
      "Sales performance varied significantly by region. \n",
      "• North America remained the strongest region, contributing over 45% of total \n",
      "revenue. \n",
      "• Europe experienced moderate growth, particularly in Germany and France. \n",
      "• Asia-Pacific showed the highest growth rate but from a smaller base. \n",
      "Future analysis should explore correlations between regional marketing spend and r\n"
     ]
    }
   ],
   "source": [
    "semantic_results = semantic_search(\n",
    "    query=query,\n",
    "    chunks=corpus,\n",
    "    embeddings=embeddings,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "print(\"SEMANTIC RESULTS\")\n",
    "display_results(semantic_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b095538",
   "metadata": {},
   "source": [
    "### Hybrid (Lexical + Semantic) Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af64b6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYBRID RESULTS\n",
      "================================================================================\n",
      "Rank: 1\n",
      "Source: data\\raw\\sample.pdf\n",
      "File type: pdf\n",
      "Metadata: {'page_index': 1, 'block_type': 'page'}\n",
      "Text preview:\n",
      "Quarterly Revenue Overview \n",
      " \n",
      "Total revenue for 2023 showed a steady upward trend, with notable acceleration in Q3. \n",
      "Quarterly revenue figures (in million USD): \n",
      "• Q1: 12.4 \n",
      "• Q2: 14.1 \n",
      "• Q3: 18.7 \n",
      "• Q4: 19.3 \n",
      "The growth observed in Q3 coincides with the launch of a new marketing campaign.\n",
      "================================================================================\n",
      "Rank: 2\n",
      "Source: data\\raw\\sample.pdf\n",
      "File type: pdf\n",
      "Metadata: {'page_index': 2, 'block_type': 'page'}\n",
      "Text preview:\n",
      "Regional Performance Analysis \n",
      "Sales performance varied significantly by region. \n",
      "• North America remained the strongest region, contributing over 45% of total \n",
      "revenue. \n",
      "• Europe experienced moderate growth, particularly in Germany and France. \n",
      "• Asia-Pacific showed the highest growth rate but from a smaller base. \n",
      "Future analysis should explore correlations between regional marketing spend and r\n",
      "================================================================================\n",
      "Rank: 3\n",
      "Source: data\\raw\\sample.pdf\n",
      "File type: pdf\n",
      "Metadata: {'page_index': 0, 'block_type': 'page'}\n",
      "Text preview:\n",
      "Annual Sales Performance Review — 2023 \n",
      "This document provides a detailed review of sales performance for the fiscal year 2023. \n",
      "The objective is to analyze revenue trends, regional performance, and category-level \n",
      "breakdowns. \n",
      "Key goals of this report include: \n",
      "• Identifying revenue growth patterns over time \n",
      "• Comparing regional performance \n",
      "• Highlighting underperforming product categories\n",
      "================================================================================\n",
      "Rank: 4\n",
      "Source: data\\raw\\sample.pptx\n",
      "File type: pptx\n",
      "Metadata: {'slide_index': 2, 'title': 'Revenue by Product Category'}\n",
      "Text preview:\n",
      "Revenue by Product Category\n",
      "================================================================================\n",
      "Rank: 5\n",
      "Source: data\\raw\\sample.pptx\n",
      "File type: pptx\n",
      "Metadata: {'slide_index': 1, 'title': 'Top Performing Products'}\n",
      "Text preview:\n",
      "Top Performing Products\n",
      "Product A showed consistent growth\n",
      "Product B peaked in mid-year\n",
      "Product C underperformed expectations\n"
     ]
    }
   ],
   "source": [
    "hybrid_results = hybrid_search(\n",
    "    query=query,\n",
    "    chunks=corpus,\n",
    "    embeddings=embeddings,\n",
    "    alpha=0.6,   # semantic weight\n",
    "    beta=0.4,    # lexical weight\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "print(\"HYBRID RESULTS\")\n",
    "display_results(hybrid_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ac3f3b",
   "metadata": {},
   "source": [
    "**What the above demonstrates is:**\n",
    "\n",
    "* ✅ Multi-format document ingestion (PDF, DOCX, PPTX, CSV, TXT)\n",
    "* ✅ Structure-aware chunking aligned with document semantics\n",
    "* ✅ Stable chunk identity across ingestion and retrieval stages\n",
    "* ✅ Corpus-level retrieval over heterogeneous sources\n",
    "* ✅ Lexical, semantic, and hybrid ranking strategies operating on the same corpus\n",
    "\n",
    "Taken together, these components constitute a **real, albeit rudimentary, Information Retrieval (IR) system**.\n",
    "While it does not yet include persistence, re-ranking, or answer synthesis, it faithfully implements the core retrieval pipeline that underpins modern RAG and search systems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c64f558",
   "metadata": {},
   "source": [
    "## Making a simple RAG system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29baa89",
   "metadata": {},
   "source": [
    "### Testing the models \n",
    "\n",
    "Before adding the models to the pipeline lets first see whether the hardware can run the model standalone. If successfull and the pipeline still fails it will be easier to debug as this testing should confrim that the model works as intented and problem should be somewhere else in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53cdfa80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cosine similarity is a measure of the cosine of the angle between two non-zero vectors in a multi-dimensional space, indicating the degree of similarity between the vectors.\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=\"models/Phi-3-mini-4k-instruct-q4.gguf\",\n",
    "    n_gpu_layers=20,\n",
    "    n_ctx=2048,\n",
    "    verbose=False,  # optional: hides the huge logs\n",
    ")\n",
    "\n",
    "response = llm.create_chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise technical assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain cosine similarity in one sentence.\"}\n",
    "    ],\n",
    "    max_tokens=64,\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "583e9f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cosine similarity is a measure of the degree to which two non-zero vectors point in the same direction in a multi-dimensional space, with the result being a value between -1 and 1 that represents the strength and direction of the similarity.\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "mistral = Llama(\n",
    "    model_path=\"models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n",
    "    n_gpu_layers=10,   # start at 10~14 go up/down as required; reduce if OOM\n",
    "    n_ctx=2048,\n",
    "    verbose = False\n",
    ")\n",
    "\n",
    "response = mistral.create_chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise technical assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain cosine similarity in one sentence.\"}\n",
    "    ],\n",
    "    max_tokens=64,\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53978f8",
   "metadata": {},
   "source": [
    "### Combining both models in a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41d841fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from llama_cpp import Llama\n",
    "\n",
    "MODEL_CONFIGS = {\n",
    "    \"phi3\": {\n",
    "        \"path\": \"models/Phi-3-mini-4k-instruct-q4.gguf\",\n",
    "        \"n_ctx\": 4096, # 2048\n",
    "        \"default_gpu_layers\": 12\n",
    "    },\n",
    "    \"mistral7b\": {\n",
    "        \"path\": \"models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n",
    "        \"n_ctx\": 6000, # 2048\n",
    "        \"default_gpu_layers\": 14\n",
    "    }\n",
    "}\n",
    "\n",
    "def run_llm_inference(\n",
    "    prompt: str,\n",
    "    model_name: Literal[\"phi3\", \"mistral7b\"] = \"phi3\",\n",
    "    max_tokens: int = 256,\n",
    "    temperature: float = 0.2,\n",
    "    system_prompt: str = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Run inference using a selected lightweight LLM.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prompt : str\n",
    "        The user question or instruction to answer.\n",
    "    model_name : {'phi3', 'mistral7b'}\n",
    "        - 'phi3': Phi-3-mini-4k-instruct (lighter, faster)\n",
    "        - 'mistral7b': Mistral-7B-Instruct-v0.2-Q4_K_M (larger, stronger)\n",
    "    max_tokens : int\n",
    "        Upper bound on generation length. Keep moderate to avoid OOM.\n",
    "    temperature : float\n",
    "        Sampling temperature; lower is more deterministic.\n",
    "    system_prompt : str | None\n",
    "        Instructions that govern assistant behavior.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The assistant’s generated text.\n",
    "    \"\"\"\n",
    "\n",
    "    if model_name not in MODEL_CONFIGS:\n",
    "        raise ValueError(\n",
    "            f\"Unsupported model '{model_name}'. \"\n",
    "            f\"Choose 'phi3' or 'mistral7b'.\"\n",
    "        )\n",
    "\n",
    "    config = MODEL_CONFIGS[model_name]\n",
    "\n",
    "    # Default system prompt if not provided\n",
    "    if system_prompt is None:\n",
    "        system_prompt = (\n",
    "            \"You are a helpful assistant. \"\n",
    "            \"Use context responsibly and answer the query clearly.\"\n",
    "        )\n",
    "\n",
    "    llm = Llama(\n",
    "        model_path=config[\"path\"],\n",
    "        n_gpu_layers=config[\"default_gpu_layers\"],\n",
    "        n_ctx=config[\"n_ctx\"],\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # Use chat API for both instruct models\n",
    "    response = llm.create_chat_completion(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "    \n",
    "    )\n",
    "\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "609cfe4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Phi-3-mini ===\n",
      " Cosine similarity is a measure used to calculate the similarity between two non-zero vectors of an inner product space. It is most commonly applied in the context of text analysis, where the vectors represent the frequency of words in documents. The cosine similarity is calculated as the cosine of the angle between the two vectors, which is a measure of how closely related the two vectors are in terms of their orientation.\n",
      "\n",
      "The formula for cosine similarity between two vectors A and B is:\n",
      "\n",
      "cosine_similarity(A, B) = (A • B) / (||A|| * ||B||)\n",
      "\n",
      "where:\n",
      "- A • B is the dot product of vectors A and B,\n",
      "- ||A|| is the magnitude (or Euclidean norm) of vector A,\n",
      "- ||B|| is the magnitude of vector B.\n",
      "\n",
      "The result of cosine similarity is a value between -1 and 1. A cosine similarity of 1 indicates that the vectors are identical, a value of 0 indicates orthogonality (no similarity), and a value of -1 indicates that the vectors are diametrically opposed.\n",
      "\n",
      "Here's a simple Python function to calculate the cosine similarity between two vectors:\n",
      "=== Mistral-7B-Instruct ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (6000) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cosine similarity is a measure of the similarity between two non-zero vectors of an inner product space, such as Euclidean space. It is used to determine the degree of alignment between the vectors, and is often used in text analysis, information retrieval, and machine learning to compare the similarity of documents or data points.\n",
      "\n",
      "The cosine similarity is calculated by taking the dot product of the two vectors and dividing it by the product of the magnitudes (lengths) of the vectors. The result is a value between -1 and 1, where 1 indicates identical vectors, 0 indicates orthogonal (perpendicular) vectors, and -1 indicates opposite vectors. A value close to 1 indicates high similarity, while a value close to -1 indicates low similarity.\n",
      "\n",
      "The formula for cosine similarity is:\n",
      "\n",
      "cosine_similarity = (A · B) / (||A|| * ||B||)\n",
      "\n",
      "where A and B are the input vectors, \"·\" represents the dot product, and \"||\" represents the magnitude (or Euclidean norm) of the vector.\n"
     ]
    }
   ],
   "source": [
    "# Testing both models\n",
    "\n",
    "print(\"=== Phi-3-mini ===\")\n",
    "print(run_llm_inference(\"What is cosine similarity?\", model_name=\"phi3\"))\n",
    "\n",
    "print(\"=== Mistral-7B-Instruct ===\")\n",
    "print(run_llm_inference(\"What is cosine similarity?\", model_name=\"mistral7b\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d21f280",
   "metadata": {},
   "source": [
    "### Context Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6df9c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_context(results, max_tokens: int = 3000) -> str:\n",
    "    \"\"\"\n",
    "    Assemble retrieved chunks into a single context string for LLM inference.\n",
    "\n",
    "    This function concatenates the top-ranked retrieved chunks while enforcing\n",
    "    an approximate token budget. The default limit (3000 tokens) is chosen to\n",
    "    accommodate small-to-mid sized instruction-tuned LLMs running on\n",
    "    consumer-grade GPUs (e.g., RTX 3060 with ~6GB VRAM).\n",
    "\n",
    "    Why this constraint exists:\n",
    "    - Larger context windows increase VRAM usage and inference latency.\n",
    "    - Quantized 7B-class models typically operate safely within a 2k–4k token window.\n",
    "    - Exceeding this budget may cause OOM errors or severe slowdowns.\n",
    "\n",
    "    Advanced users with more VRAM (or CPU-based inference) may safely increase\n",
    "    this value, provided their selected model supports larger context windows.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results : list of dict\n",
    "        Ranked retrieval results. Each entry must contain:\n",
    "        - \"text\": chunk text\n",
    "        - \"source\": source document identifier\n",
    "        - \"id\": stable chunk identifier\n",
    "\n",
    "    max_tokens : int, optional (default=3000)\n",
    "        Approximate token budget for the assembled context.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A single formatted context string suitable for RAG-style prompting.\n",
    "    \"\"\"\n",
    "\n",
    "    context_blocks = []\n",
    "    token_count = 0\n",
    "\n",
    "    for chunk in results:\n",
    "        text = chunk[\"text\"]\n",
    "        source = chunk.get(\"source\", \"unknown\")\n",
    "        chunk_id = chunk.get(\"id\", \"unknown\")\n",
    "\n",
    "        # Approximate token count (1 token ≈ 0.75 words is model-dependent)\n",
    "        approx_tokens = len(text.split())\n",
    "\n",
    "        if token_count + approx_tokens > max_tokens:\n",
    "            break\n",
    "\n",
    "        block = (\n",
    "            f\"[Source: {source} | Chunk ID: {chunk_id}]\\n\"\n",
    "            f\"{text}\\n\"\n",
    "        )\n",
    "\n",
    "        context_blocks.append(block)\n",
    "        token_count += approx_tokens\n",
    "\n",
    "    return \"\\n\".join(context_blocks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838274c3",
   "metadata": {},
   "source": [
    "## LLM Answer Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59497c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_llm_answer(\n",
    "    context: str,\n",
    "    query: str,\n",
    "    model: str,\n",
    "    system_prompt: str = (\n",
    "        \"You are a technical assistant.\\n\"\n",
    "        \"Answer the question using ONLY the provided context.\\n\"\n",
    "        \"If the answer cannot be found in the context, say so explicitly.\"\n",
    "    ),\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate an answer using a lightweight, VRAM-constrained LLM for RAG.\n",
    "\n",
    "    This function performs grounded generation by combining:\n",
    "    - a system prompt (behavioral constraint),\n",
    "    - retrieved context (knowledge grounding),\n",
    "    - and a user query.\n",
    "\n",
    "    The model must be explicitly selected from the supported options to ensure\n",
    "    predictable VRAM usage and inference behavior.\n",
    "\n",
    "    Supported models:\n",
    "    - \"mistral7b\": higher reasoning quality, ~4–5GB VRAM (4-bit)\n",
    "    - \"phi3\": lower VRAM, faster inference, weaker reasoning\n",
    "    - defaults to \"phi3\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    context : str\n",
    "        Assembled retrieval context produced by `assemble_context`.\n",
    "\n",
    "    query : str\n",
    "        User query to be answered.\n",
    "\n",
    "    model : str\n",
    "        Model identifier. Must be one of:\n",
    "        - \"mistral7b\"\n",
    "        - \"phi3\"\n",
    "\n",
    "    system_prompt : str, optional\n",
    "        Instruction to guide model behavior. Defaults to a strict\n",
    "        context-grounded RAG prompt.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Generated answer text.\n",
    "    \"\"\"\n",
    "\n",
    "    if model not in MODEL_CONFIGS:\n",
    "        raise ValueError(\n",
    "            f\"Unsupported model '{model}'. \"\n",
    "            f\"Choose from: {list(MODEL_CONFIGS.keys())}\"\n",
    "        )\n",
    "\n",
    "\n",
    "    if not context.strip():\n",
    "        return (\n",
    "        \"No relevant information was found in the retrieved documents \"\n",
    "        \"to answer this question.\"\n",
    "    )\n",
    "\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "{system_prompt}\n",
    "\n",
    "Context:\n",
    "---------\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "---------\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "    # Placeholder for actual inference call\n",
    "    # Example options:\n",
    "    # - llama_cpp.Llama(...)\n",
    "    # - subprocess call to llama.cpp binary\n",
    "    # - local wrapper around gguf inference\n",
    "\n",
    "    if len(prompt) > 12000:\n",
    "        raise ValueError(\"Prompt too long for configured context window.\")\n",
    "\n",
    "    answer = run_llm_inference(\n",
    "        prompt=prompt,\n",
    "        model_name=model,\n",
    "    )\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee41e8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks in corpus: 13\n",
      "{'id': '6dbe80d88422232e', 'source': 'data\\\\raw\\\\sample.pdf', 'file_type': 'pdf', 'text': 'Annual Sales Performance Review — 2023 \\nThis document provides a detailed review of sales performance for the fiscal year 2023. \\nThe objective is to analyze revenue trends, regional performance, and category-level \\nbreakdowns. \\nKey goals of this report include: \\n• Identifying revenue growth patterns over time \\n• Comparing regional performance \\n• Highlighting underperforming product categories', 'metadata': {'page_index': 0, 'block_type': 'page'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\islam\\AppData\\Local\\Temp\\ipykernel_39280\\1653241032.py:164: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(sample, errors=\"raise\")\n",
      "C:\\Users\\islam\\AppData\\Local\\Temp\\ipykernel_39280\\1653241032.py:164: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(sample, errors=\"raise\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c627dfb3b99143e3a4a282ef846f2981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1 Build Corpus\n",
    "file_paths = [\n",
    "    \"data/raw/sample.pdf\",\n",
    "    \"data/raw/sample.docx\",\n",
    "    \"data/raw/sample.pptx\",\n",
    "    \"data/raw/sample.csv\",\n",
    "    \n",
    "]\n",
    "\n",
    "corpus = build_corpus(file_paths)\n",
    "\n",
    "print(f\"Total chunks in corpus: {len(corpus)}\")\n",
    "\n",
    "print(corpus[0])\n",
    "\n",
    "# 2 Embed Chunks\n",
    "embeddings = embed_chunks(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b6ac2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (6000) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL ANSWER ===\n",
      " According to the context provided, the revenue on 01-01-23 for the North America region in the Electronics product category was $1,200,000 and the units sold were 3,400.\n"
     ]
    }
   ],
   "source": [
    "query = \"What were the revenue and units sold on 01-01-23\"\n",
    "\n",
    "results = hybrid_search(\n",
    "    query=query,\n",
    "    chunks=corpus,\n",
    "    embeddings=embeddings,\n",
    "    alpha=0.6,   # semantic weight\n",
    "    beta=0.4,    # lexical weight\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "\n",
    "context = assemble_context(results)\n",
    "\n",
    "answer = generate_llm_answer(\n",
    "    context=context,\n",
    "    query=query,\n",
    "    model=\"mistral7b\",  # \"mistral7b\" | \"phi3\"\n",
    ")\n",
    "print(\"\\n=== FINAL ANSWER ===\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8c38eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing generation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile generation.py\n",
    "\n",
    "from typing import Literal\n",
    "from llama_cpp import Llama\n",
    "\n",
    "MODEL_CONFIGS = {\n",
    "    \"phi3\": {\n",
    "        \"path\": \"models/Phi-3-mini-4k-instruct-q4.gguf\",\n",
    "        \"n_ctx\": 4096, # 2048\n",
    "        \"default_gpu_layers\": 12\n",
    "    },\n",
    "    \"mistral7b\": {\n",
    "        \"path\": \"models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n",
    "        \"n_ctx\": 6000, # 2048\n",
    "        \"default_gpu_layers\": 14\n",
    "    }\n",
    "}\n",
    "\n",
    "def run_llm_inference(\n",
    "    prompt: str,\n",
    "    model_name: Literal[\"phi3\", \"mistral7b\"] = \"phi3\",\n",
    "    max_tokens: int = 256,\n",
    "    temperature: float = 0.2,\n",
    "    system_prompt: str = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Run inference using a selected lightweight LLM.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prompt : str\n",
    "        The user question or instruction to answer.\n",
    "    model_name : {'phi3', 'mistral7b'}\n",
    "        - 'phi3': Phi-3-mini-4k-instruct (lighter, faster)\n",
    "        - 'mistral7b': Mistral-7B-Instruct-v0.2-Q4_K_M (larger, stronger)\n",
    "    max_tokens : int\n",
    "        Upper bound on generation length. Keep moderate to avoid OOM.\n",
    "    temperature : float\n",
    "        Sampling temperature; lower is more deterministic.\n",
    "    system_prompt : str | None\n",
    "        Instructions that govern assistant behavior.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The assistant’s generated text.\n",
    "    \"\"\"\n",
    "\n",
    "    if model_name not in MODEL_CONFIGS:\n",
    "        raise ValueError(\n",
    "            f\"Unsupported model '{model_name}'. \"\n",
    "            f\"Choose 'phi3' or 'mistral7b'.\"\n",
    "        )\n",
    "\n",
    "    config = MODEL_CONFIGS[model_name]\n",
    "\n",
    "    # Default system prompt if not provided\n",
    "    if system_prompt is None:\n",
    "        system_prompt = (\n",
    "            \"You are a helpful assistant. \"\n",
    "            \"Use context responsibly and answer the query clearly.\"\n",
    "        )\n",
    "\n",
    "    llm = Llama(\n",
    "        model_path=config[\"path\"],\n",
    "        n_gpu_layers=config[\"default_gpu_layers\"],\n",
    "        n_ctx=config[\"n_ctx\"],\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # Use chat API for both instruct models\n",
    "    response = llm.create_chat_completion(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "    \n",
    "    )\n",
    "\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "def assemble_context(results, max_tokens: int = 3000) -> str:\n",
    "    \"\"\"\n",
    "    Assemble retrieved chunks into a single context string for LLM inference.\n",
    "\n",
    "    This function concatenates the top-ranked retrieved chunks while enforcing\n",
    "    an approximate token budget. The default limit (3000 tokens) is chosen to\n",
    "    accommodate small-to-mid sized instruction-tuned LLMs running on\n",
    "    consumer-grade GPUs (e.g., RTX 3060 with ~6GB VRAM).\n",
    "\n",
    "    Why this constraint exists:\n",
    "    - Larger context windows increase VRAM usage and inference latency.\n",
    "    - Quantized 7B-class models typically operate safely within a 2k–4k token window.\n",
    "    - Exceeding this budget may cause OOM errors or severe slowdowns.\n",
    "\n",
    "    Advanced users with more VRAM (or CPU-based inference) may safely increase\n",
    "    this value, provided their selected model supports larger context windows.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results : list of dict\n",
    "        Ranked retrieval results. Each entry must contain:\n",
    "        - \"text\": chunk text\n",
    "        - \"source\": source document identifier\n",
    "        - \"id\": stable chunk identifier\n",
    "\n",
    "    max_tokens : int, optional (default=3000)\n",
    "        Approximate token budget for the assembled context.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A single formatted context string suitable for RAG-style prompting.\n",
    "    \"\"\"\n",
    "\n",
    "    context_blocks = []\n",
    "    token_count = 0\n",
    "\n",
    "    for chunk in results:\n",
    "        text = chunk[\"text\"]\n",
    "        source = chunk.get(\"source\", \"unknown\")\n",
    "        chunk_id = chunk.get(\"id\", \"unknown\")\n",
    "\n",
    "        # Approximate token count (1 token ≈ 0.75 words is model-dependent)\n",
    "        approx_tokens = len(text.split())\n",
    "\n",
    "        if token_count + approx_tokens > max_tokens:\n",
    "            break\n",
    "\n",
    "        block = (\n",
    "            f\"[Source: {source} | Chunk ID: {chunk_id}]\\n\"\n",
    "            f\"{text}\\n\"\n",
    "        )\n",
    "\n",
    "        context_blocks.append(block)\n",
    "        token_count += approx_tokens\n",
    "\n",
    "    return \"\\n\".join(context_blocks)\n",
    "\n",
    "def generate_llm_answer(\n",
    "    context: str,\n",
    "    query: str,\n",
    "    model: str,\n",
    "    system_prompt: str = (\n",
    "        \"You are a technical assistant.\\n\"\n",
    "        \"Answer the question using ONLY the provided context.\\n\"\n",
    "        \"If the answer cannot be found in the context, say so explicitly.\"\n",
    "    ),\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate an answer using a lightweight, VRAM-constrained LLM for RAG.\n",
    "\n",
    "    This function performs grounded generation by combining:\n",
    "    - a system prompt (behavioral constraint),\n",
    "    - retrieved context (knowledge grounding),\n",
    "    - and a user query.\n",
    "\n",
    "    The model must be explicitly selected from the supported options to ensure\n",
    "    predictable VRAM usage and inference behavior.\n",
    "\n",
    "    Supported models:\n",
    "    - \"mistral7b\": higher reasoning quality, ~4–5GB VRAM (4-bit)\n",
    "    - \"phi3\": lower VRAM, faster inference, weaker reasoning\n",
    "    - defaults to \"phi3\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    context : str\n",
    "        Assembled retrieval context produced by `assemble_context`.\n",
    "\n",
    "    query : str\n",
    "        User query to be answered.\n",
    "\n",
    "    model : str\n",
    "        Model identifier. Must be one of:\n",
    "        - \"mistral7b\"\n",
    "        - \"phi3\"\n",
    "\n",
    "    system_prompt : str, optional\n",
    "        Instruction to guide model behavior. Defaults to a strict\n",
    "        context-grounded RAG prompt.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Generated answer text.\n",
    "    \"\"\"\n",
    "\n",
    "    if model not in MODEL_CONFIGS:\n",
    "        raise ValueError(\n",
    "            f\"Unsupported model '{model}'. \"\n",
    "            f\"Choose from: {list(MODEL_CONFIGS.keys())}\"\n",
    "        )\n",
    "\n",
    "\n",
    "    if not context.strip():\n",
    "        return (\n",
    "        \"No relevant information was found in the retrieved documents \"\n",
    "        \"to answer this question.\"\n",
    "    )\n",
    "\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "{system_prompt}\n",
    "\n",
    "Context:\n",
    "---------\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "---------\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "    \n",
    "    # Inference backend is currently llama.cpp via llama_cpp.\n",
    "    # This function can be extended to support alternative backends\n",
    "    # (e.g., vLLM, Triton, remote APIs) without changing call sites.\n",
    "\n",
    "\n",
    "    if len(prompt) > 12000:\n",
    "        raise ValueError(\"Prompt too long for configured context window.\")\n",
    "\n",
    "    answer = run_llm_inference(\n",
    "        prompt=prompt,\n",
    "        model_name=model,\n",
    "    )\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eef81b",
   "metadata": {},
   "source": [
    "The above is simply taking the functions created for LLM answer generation and saving it into a .py file to modularize the workflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8193811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting state.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile state.py\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from llama_cpp import Llama\n",
    "\n",
    "from core.ingestion import ingest_file\n",
    "from core.chunking import chunk_document\n",
    "from core.retrieval import embed_chunks\n",
    "from generation import MODEL_CONFIGS\n",
    "\n",
    "\n",
    "DATA_DIR = Path(\"data_state\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "CORPUS_PATH = DATA_DIR / \"corpus.pkl\"\n",
    "EMBEDDINGS_PATH = DATA_DIR / \"embeddings.npy\"\n",
    "\n",
    "\n",
    "_corpus: Optional[List[Dict]] = None\n",
    "_embeddings: Optional[np.ndarray] = None\n",
    "_embedding_model: Optional[SentenceTransformer] = None\n",
    "_llm_cache: Dict[str, Llama] = {}\n",
    "_last_answer: Optional[str] = None\n",
    "\n",
    "\n",
    "def load_state():\n",
    "    global _corpus, _embeddings\n",
    "\n",
    "    if CORPUS_PATH.exists():\n",
    "        with open(CORPUS_PATH, \"rb\") as f:\n",
    "            _corpus = pickle.load(f)\n",
    "\n",
    "    if EMBEDDINGS_PATH.exists():\n",
    "        _embeddings = np.load(EMBEDDINGS_PATH)\n",
    "\n",
    "    return _corpus, _embeddings\n",
    "\n",
    "\n",
    "def save_state(corpus, embeddings):\n",
    "    with open(CORPUS_PATH, \"wb\") as f:\n",
    "        pickle.dump(corpus, f)\n",
    "    np.save(EMBEDDINGS_PATH, embeddings)\n",
    "\n",
    "\n",
    "def get_embedding_model():\n",
    "    global _embedding_model\n",
    "    if _embedding_model is None:\n",
    "        _embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    return _embedding_model\n",
    "\n",
    "\n",
    "def build_or_update_corpus(paths: List[Path], rebuild=False):\n",
    "    global _corpus, _embeddings\n",
    "\n",
    "    if rebuild or _corpus is None:\n",
    "        _corpus = []\n",
    "        _embeddings = None\n",
    "\n",
    "    existing_sources = {c[\"source\"] for c in _corpus} if _corpus else set()\n",
    "    new_chunks = []\n",
    "\n",
    "    for path in paths:\n",
    "        if str(path) in existing_sources:\n",
    "            continue\n",
    "        doc = ingest_file(str(path))\n",
    "        new_chunks.extend(chunk_document(doc))\n",
    "\n",
    "    if not new_chunks:\n",
    "        return _corpus\n",
    "\n",
    "    model = get_embedding_model()\n",
    "    new_embeddings = model.encode([c[\"text\"] for c in new_chunks])\n",
    "\n",
    "    _embeddings = (\n",
    "        new_embeddings if _embeddings is None\n",
    "        else np.vstack([_embeddings, new_embeddings])\n",
    "    )\n",
    "\n",
    "    _corpus.extend(new_chunks)\n",
    "    save_state(_corpus, _embeddings)\n",
    "    return _corpus\n",
    "\n",
    "\n",
    "def get_corpus():\n",
    "    global _corpus\n",
    "    if _corpus is None:\n",
    "        load_state()\n",
    "    return _corpus\n",
    "\n",
    "\n",
    "def get_embeddings():\n",
    "    global _embeddings\n",
    "    if _embeddings is None:\n",
    "        load_state()\n",
    "    return _embeddings\n",
    "\n",
    "\n",
    "def get_llm(model_name: str) -> Llama:\n",
    "    if model_name not in MODEL_CONFIGS:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "\n",
    "    if model_name not in _llm_cache:\n",
    "        cfg = MODEL_CONFIGS[model_name]\n",
    "        _llm_cache[model_name] = Llama(\n",
    "            model_path=cfg[\"path\"],\n",
    "            n_ctx=cfg[\"n_ctx\"],\n",
    "            n_gpu_layers=cfg[\"default_gpu_layers\"],\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "    return _llm_cache[model_name]\n",
    "\n",
    "\n",
    "def get_last_answer():\n",
    "    return _last_answer\n",
    "\n",
    "\n",
    "def set_last_answer(answer: str):\n",
    "    global _last_answer\n",
    "    _last_answer = answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db37da38",
   "metadata": {},
   "source": [
    "The state file has some new functions namely to keep track of a few things to improve the efficiency of the workflow. First the corpus nad embeddings are created from the directory (once) and updated only if new files are detected. Another important addition is the tracking of the current answer generated by the LLM and storing in a state variable called \"_last_answer\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e859c668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from state import (\n",
    "    build_or_update_corpus,\n",
    "    get_corpus,\n",
    "    get_embeddings,\n",
    "    get_last_answer,\n",
    "    set_last_answer,\n",
    "    get_llm\n",
    ")\n",
    "\n",
    "from core.retrieval import hybrid_search\n",
    "from generation import assemble_context, generate_llm_answer\n",
    "\n",
    "\n",
    "def collect_files(paths: List[str]) -> List[Path]:\n",
    "    files = []\n",
    "    for p in paths:\n",
    "        path = Path(p)\n",
    "        if path.is_dir():\n",
    "            files.extend(f for f in path.rglob(\"*\") if f.is_file())\n",
    "        else:\n",
    "            files.append(path)\n",
    "    return files\n",
    "\n",
    "\n",
    "def estimate_tokens(llm, text: str) -> int:\n",
    "    return len(llm.tokenize(text.encode(\"utf-8\")))\n",
    "\n",
    "\n",
    "def answer_query(query: str, model=\"mistral7b\", top_k=3):\n",
    "    corpus = get_corpus()\n",
    "    embeddings = get_embeddings()\n",
    "\n",
    "    results = hybrid_search(\n",
    "        query=query,\n",
    "        chunks=corpus,\n",
    "        embeddings=embeddings,\n",
    "        top_k=top_k\n",
    "    )\n",
    "\n",
    "    context = assemble_context(results)\n",
    "\n",
    "    prev = get_last_answer()\n",
    "    if prev:\n",
    "        context = f\"[Previous Answer]\\n{prev}\\n\\n{context}\"\n",
    "\n",
    "    answer = generate_llm_answer(\n",
    "        context=context,\n",
    "        query=query,\n",
    "        model=model\n",
    "    )\n",
    "\n",
    "    set_last_answer(answer)\n",
    "\n",
    "    llm = get_llm(model)\n",
    "    tokens = {\n",
    "        \"query\": estimate_tokens(llm, query),\n",
    "        \"context\": estimate_tokens(llm, context),\n",
    "        \"answer\": estimate_tokens(llm, answer),\n",
    "    }\n",
    "    tokens[\"total\"] = sum(tokens.values())\n",
    "\n",
    "    return answer, tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4116ab",
   "metadata": {},
   "source": [
    "A simple app.py for using the model using CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd6b469c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing streamlit_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile streamlit_app.py\n",
    "\n",
    "import streamlit as st\n",
    "from app import answer_query, collect_files\n",
    "from state import build_or_update_corpus, get_corpus\n",
    "\n",
    "\n",
    "st.set_page_config(page_title=\"Local Hybrid RAG\", layout=\"wide\")\n",
    "st.title(\"Local Hybrid RAG (Lexical + Semantic)\")\n",
    "\n",
    "\n",
    "st.sidebar.header(\"Corpus\")\n",
    "data_dir = st.sidebar.text_input(\"Data directory\", \"data/raw\")\n",
    "rebuild = st.sidebar.checkbox(\"Rebuild corpus\")\n",
    "\n",
    "if st.sidebar.button(\"Ingest\"):\n",
    "    files = collect_files([data_dir])\n",
    "    build_or_update_corpus(files, rebuild=rebuild)\n",
    "    st.sidebar.success(\"Corpus ready\")\n",
    "\n",
    "corpus = get_corpus()\n",
    "if corpus:\n",
    "    st.sidebar.write(f\"Chunks: {len(corpus)}\")\n",
    "\n",
    "\n",
    "query = st.text_input(\"Query\")\n",
    "\n",
    "col1, col2 = st.columns(2)\n",
    "with col1:\n",
    "    model = st.selectbox(\"Model\", [\"mistral7b\"])\n",
    "with col2:\n",
    "    top_k = st.slider(\"Top-K\", 1, 10, 3)\n",
    "\n",
    "if st.button(\"Run\") and query:\n",
    "    with st.spinner(\"Running hybrid retrieval...\"):\n",
    "        answer, tokens = answer_query(query, model=model, top_k=top_k)\n",
    "\n",
    "    st.subheader(\"Answer\")\n",
    "    st.write(answer)\n",
    "\n",
    "    st.subheader(\"Token Usage\")\n",
    "    st.json(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ba8cfa",
   "metadata": {},
   "source": [
    "A simple streamlit UI that combines everything done till now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docinsight-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
